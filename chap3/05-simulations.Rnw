We include some simulation results to show the validity of these extensions.

\subsection{Logistic first stage}
\label{p2:simulations:logistic}

To examine the generalized linear model first stage extension, we focus on a common variation, that of a logistic first stage.

\subsubsection{Data Generation}
\label{p2:simulations:logistic:datagen}

The covariates \(X\) are generated randomly from \(N(0,1)\), \(X \in \R_{n\times q}\). We use \(n\) = 100 and \(n\) = 1,000 for smaller and larger
sample situations. We use \(q\) = 7 and \(q\) = 17 for \(n\) = 100 and \(n\) = 1,000 respectively, due to the rule of thumb we developed in Section
\ref{p1:simulations:results:chooseq}. \(q\) describes merely the dimensions of the generated \(X\), and it can (and often will be the case) that the
response \(Y\) is generated by a data generating matrix of dimension \(n\times p\), which is a subset of \(X\), such that \(p < q\). This distinction
is why we use \(q\) to represent the dimension of \(X\) and \(p\) to represent the dimension of the data generating matrix.

\(\beta_c\) in the first stage model are drawn from \(N(0,1)\), with some \(q - p\) of the \(\beta_c\) forced to 0 to add some noise.

The choice of second stage model parameters \(\tau\) and \(\eta\) require a bit more finesse than in the linear first stage case. In those cases, the
only restriction on these parameters in a simulation was that \(\eta \in (-1,2)\), those values being chosen to restrict attention to models where the
relationship between \(X\) and \(Y_c\) and between \(X\) and \(Y_t\) are similar.

However, in the cases where the response in the not normally distributed and the first stage is a generalized linear model, there are additional
restrictions upon \(\eta\) and \(\tau\). Unlike the linear-linear restrictions (where an estimate of \(\eta\) outside of \((-1,2)\) might indicate
that there are additional model complexities that the method does not address), these restrictions are purely mathematical.

Consider the case where \(Y\) is binary and the first stage model is logistic. In this setting, \(\hat{Y}_c = \logit^{-1}(X\hat{\beta_c})\) and the left
hand side of the second stage model, \(Y_t - \hat{Y}_c\), is restricted to \([-1,1]\). For example, we cannot have that \(\tau = 1.5\) and
\(\eta > 0\), as then for all values of \(\hat{Y}_c \in [0,1]\), the right hand side never maps to \([-1,1]\). If \(\tau = 0.5\) and \(\eta = -0.4\),
then all values of \(\hat{Y}_c \in [0,1]\) maps the right hand side into \([-1, 1]\). There are some cases with partial successful matching, for
example, if \(\tau = -0.25\) and \(\eta = 0.5\), then \(\hat{Y}_c \in [0, 0.5)\) does not map into \([-1, 1]\) while \(\hat{Y}_c \in [0.5, 1]\) does.

<<echo=FALSE, fig.width=4, fig.height=4, fig.cap="Possible values of $\\eta$ and $\\tau$. The light green and light blue represent the two regions described in the text whose union covers mapping of some values of $\\logit^{-1}(X\\hat{\\beta}_c)$ to $[-1,1]$, and the red represents their intersection which maps all values of $\\logit^{-1}(X\\hat{\\beta}_c)$ to $[-1,1]$ .\\label{p2:graph:sim_logistic_values}", fig.scap="Choices of $\\eta$ and $\\tau$ with logistic first stage">>=
plot(NULL, xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5), xlab = expression(tau), ylab = expression(eta))
polygon(c(1, 1, -4), c(3, -2, 3), col = 'lightblue')
polygon(c(0, 3, 0), c(0, -3, -3), col = 'lightgreen')
polygon(c(0, 1, 1, 0), y = c(0, -1, -2, -1), col = 'red')
@

There are two regions of interest in defining, in the logistic case, feasible values of the parameters, \(\{\tau \ge 0, \eta \le -\tau\}\) and
\(\{\tau \le 1, \eta \ge - 1 - \tau\}\). The union of those two regions corresponds to values of \(\eta\) and \(\tau\) where some values of
\(\hat{Y}_c \in [0,1]\) map to \([-1, 1]\). The intersection of those regions corresponds to values of \(\eta\) and \(\tau\) where all values of
\(\hat{Y}_c \in [0,1]\) map to \([-1, 1\)]. This is visually represented in Figure \ref{p2:graph:sim_logistic_values}.

Finally, we generate success probabilities for each individual using
\begin{equation}
  \rho_i = \logit^{-1}(-X_i\beta_c) + \tau \times Z + \eta \times Z \times \logit^{-1}(-X_i\beta_c).
\end{equation}

In the control group, \(Z\) = 0 and \(\rho_i = \logit^{-1}(X_i\beta_c)\). When \(Z\) = 1 in the treatment group, we have the additional additive
effects. We truncate values of \(\rho_i\) above 1 or below 0. From this, we generate \(Y_i\) from \(\textrm{Bern}(\rho_i)\).

\subsubsection{Simulation Results}

We generate values of \(\eta\) and \(\tau\) which fall within the regions described above, and run 1,000 repetitions each, generating a coverage
percentage. These coverage percentages are plotted in Figure \ref{p2:graph:sim_logistic_100} for \(n\) = 100 and Figure
\ref{p2:graph:sim_logistic_1000} for \(n\) = 1,000.

<<echo=FALSE, fig.width=5, fig.height=5, fig.cap="Coverage over different values of $\\eta$ and $\\tau$ for $n$ = 100. The light green and light blue represent the two regions described in the text whose union covers mapping of some values of $\\logit^{-1}(X\\hat{\\beta}_c)$ to $[-1,1]$, and the red represents their intersection which maps all values of $\\logit^{-1}(X\\hat{\\beta}_c)$ to $[-1,1]$ .\\label{p2:graph:sim_logistic_100}", fig.scap="Logistic first stage performance for $n$ = 100.">>=
source("code/logisticsim100.R")

plot(NULL, xlim = c(-.5, 1.5), ylim = c(-2.5, 0.5), xlab = expression(tau), ylab = expression(eta))
polygon(c(1, 1, -4), c(3, -2, 3), col = 'lightblue')
polygon(c(0, 3, 0), c(0, -3, -3), col = 'lightgreen')
polygon(c(0, 1, 1, 0), y = c(0, -1, -2, -1), col = 'red')
text(bigsave[,1:2], labels = bigsave$perc)
@

<<r, echo=FALSE, fig.width=5, fig.height=5, fig.cap="Coverage over different values of $\\eta$ and $\\tau$ for $n$ =1,000. Coverage is less conservative in the red area, and performs much worse in the outlying regions.\\label{p2:graph:sim_logistic_1000}", fig.scap="Logistic first stage performance for $n$ = 1000.">>=
source("code/logisticsim1000.R")

plot(NULL, xlim = c(-.5, 1.5), ylim = c(-2.5, 0.5), xlab = expression(tau), ylab = expression(eta))
polygon(c(1, 1, -4), c(3, -2, 3), col = 'lightblue')
polygon(c(0, 3, 0), c(0, -3, -3), col = 'lightgreen')
polygon(c(0, 1, 1, 0), y = c(0, -1, -2, -1), col = 'red')
text(bigsave[,1:2], labels = bigsave$perc)
@

The red area, where values of \(\tau\) and \(\eta\) map all values of \(\logit^{-1}(X\beta_c)\) into \([-1,1]\), shows proper coverage which is
slightly conservative. However, the blue and green areas, where values of the parameters map some values of \(\logit^{-1}(X\beta_c)\) into \([-1,1]\),
shows poor coverage, a problem which is exacerbated with the larger \(n\). This suggests the need to be very careful if \(\hat{\eta}\) and
\(\hat{\tau}\) fall outside of the red area, as the type I error will be large.

\subsection{Clusters}
\label{p2:simulations:clusters}

Data generation follows Section \ref{p2:simulations:logistic:datagen} with a few modifications. Following the notation of Section \ref{p2:clusters},
let each of \(n\) observations belong to exactly one of \(S\) clusters and \(n_s\) observations belong to cluster \(s\). Due to the intracluster
correlation discussed in Section \ref{p2:clusters}, we require a larger sample size to obtain similar power to the non-cluster
version.\citep{guo2005small} In practice, we will use \(S\) = 10 and 100 and \(n\) = 1,000 and 4,000, examining all four pairings, to see the
difference in effect of the size of \(S\) versus the effect of the size of \(n\). We randomly assign observations to a cluster with equal probability,
so that \(\E(n_s) = n/S\). With equal probability of assigning each cluster to treatment or control, we have \(\E\left(\sum Z_i\right) = n/2\), so we
use \(q\) = 13 and 22 for \(n\) = 1,000 and 4,000 respectively, again following the rule of thumb from Section
\ref{p1:simulations:results:chooseq}. We do not include any cluster-level effects.

For choices of \(\eta\) and \(\tau\), the addition of clusters does not affect results from the most basic case, so we merely limit \(\eta\) to
\((-1, 2)\).

\subsubsection{Cluster Simulation Results}
\label{p2:simulations:clusters:results}

For each combination of \((S, n) \in \left\{(10, 100) \times (1000, 4000)\right\}\), we run 1,000 replications as described above and examine
coverage. The results are plotted in Figure \ref{p2:graph:sim_cluster}. With the largest configuration, we see proper 95\% coverage. As \(S\) or \(n\)
decrease, the coverage drops. The effect of \(S\) decreasing is much more substantial. Since shrinking \(n\) has a very minor impact, we can
interpolate implying that the size of \(\E(n_s)\) also plays a small role.

An additional note is that because \(n\) is so large in these simulations, the bias we observed in the linear variation of the corrected PBPH
vanished.

<<echo=FALSE, results="asis", fig.height=5, fig.width=5, fig.cap="Coverage over $\\eta$ with $S$ clusters and sample size $n$. As either $n$ or $S$ increases, coverage increases, though the effect of $S$ is more extreme.\\label{p2:graph:sim_cluster}", fig.scap="Performance with clusters">>=
source("code/clustersim_1000_10.R")
source("code/clustersim_1000_100.R")
source("code/clustersim_4000_10.R")
source("code/clustersim_4000_100.R")

s <- cbind(clus_1000_10[,c("truth", "estimate", "perc_cov")],
           clus_1000_100[,c("estimate", "perc_cov")],
           clus_4000_10[,c("estimate", "perc_cov")],
           clus_4000_100[,c("estimate", "perc_cov")])

plot(s[,3] ~ s$truth, ylim = c(75, 100), type ='l',
     xlab = expression(eta),
     ylab = "Coverage (%)",)
lines(s[,5] ~ s$truth, col = 2)
lines(s[,7] ~ s$truth, col = 3)
lines(s[,9] ~ s$truth, col = 4)
abline(h = 95, col='lightgrey', lty = 2)
legend("bottomleft", legend = c("n = 1000, S = 10",
                                "n = 1000, S = 100",
                                "n = 4000, S = 10",
                                "n = 4000, S = 100"),
       lty = c(1,1,1,1), col=1:4)
@

\subsection{Revisiting \citet{gine2012credit} with Clusters}
\label{p2:simulations:gine}

We revisit \citet{gine2012credit}, the paper which motivated this work (see Section \ref{p1:motivation}). The authors performed an uncorrected PBPH
method to determine whether fingerprinting farmers applying for loans in rural Malawi improved repayment rates, and whether the improved repayment was
greatest for those most likely to default. The paper found affirmative answers to both questions.

In Section \ref{p1:simulations:gine}, we re-analyzed their results using the corrected PBPH and confirmed their results, with the caveat that the
confidence interval we generated almost covered -1. An interaction coefficient of -1 would indicate that there is no relationship between the
potential responses.

In \citet{gine2012credit}, the unit of randomization was not farmer but ``club'', a collection of farmers who share risk. We ignored this complication
in Section \ref{p1:simulations:gine}, but with the addition of allowing clustered randomized trials, we can include the clubs. The results are shown
in Table \ref{p2:tab:sim:gine:clust}.

<<echo=FALSE, results="asis">>=
source("code/gine_clusters.R")

xt <- xtable(ginecorr,
             caption = "Comparison of uncorrected and corrected confidence intervals, adjusting for clustered randomized trials.",
             label = "p2:tab:sim:gine:clust",
             align="cccc",
             digits=3)

print(xt,
      sanitize.text.function=function(x){x},
      comment = FALSE)
@

With proper handling of the clubs, we now see a confidence interval that does cover -1. This suggests the need to revisit results, as no relationship
between potential responses is a negative result.
