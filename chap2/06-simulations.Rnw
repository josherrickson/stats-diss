
We first show that our approaches to hypothesis testing and confidence interval by test inversion produce proper coverage. We examine the existence of
infinite confidence regions and suggest some guidelines for avoiding them. We then return to the \citet{gine2012credit} and compare uncorrected PBPH
vs corrected PBPH results.

\subsection{Data Generation}
\label{p1:simulations:datagen}

For all simulations in this section, we use a generalized data generation method which is described here. This methods allows us to specify parameters
of interest, such as \(\eta\), and also incidental parameters like \(\tau\) and nuisance parameters like \(\sigma^2\).

The covariates \(X\) are generated randomly from a Normal distribution, generally \(N(0,1)\), and \(X \in \R_{n\times q}\). \(n\) represents the total
number of observations, and we will typically use \(n\) = 100 and \(n\) = 1,000 to represent a ``small sample size'' and ``large sample size''
respectively. \(q\) represents the number of covariates. Note that the \(q\) here describes merely the dimensions of the generated \(X\), and it can
(and often will be the case) that the response \(Y\) is generated by a data generating matrix of dimension \(n\times p\), which is a subset of \(X\),
such that \(p < q\). This distinction is why we use \(q\) to represent the dimension of \(X\) and \(p\) to represent the dimension of the data
generating matrix.

Asymptotic theory for sandwich estimators allows \(q\) to grow along with \(n\), establishing consistency results paralleling those of the classical
fixed-\(p\) development under the assumption that \(q^2 \log(q)\) is \(o(n)\), i.e.  \(q^2 \log(q)\) is small in relation to
\(n\).\citep{he2000parameters, portnoy1984asymptotic} We generate a finite sample rule of thumb in Section \ref{p1:simulations:results:chooseq}, which
translates into choices of \(q\) = 7 and \(q\) = 17 for \(n\) = 100 and \(n\) = 1,000 respectively.

The treatment variable, \(Z \in \{0,1\}\), is randomly assigned with some probability \(p_Z\) of being assigned to treatment (\(Z\) = 1).

To generate responses, we need to specify four parameters, \(\beta_c, \tau, \eta\) and \(\sigma^2\).

\(\beta_c\) represent the coefficients on the data generating matrix used to generate the response. As before, typically \(p\) of these \(\beta_c\)
will be non-zero, while the remaining \(q-p\) are 0. We use this sparsity specifically to examine under-fitting and over-fitting, but more generally
to avoid accusations of using only oracle models. Non-zero \(\beta_c\)'s are randomly drawn from \(N(0,1)\).

\(\tau\), representing the additive treatment effect, is in theory allowed to take any value, but in practice we restrict it. Due to \(X\) being
\(N(0,1\)), we chose \(\tau\) on the same scale, either manually fixed or drawn randomly \(N(0,1)\). In real-world data situations where \(\tau\) is
on a larger scale than \(X\), this method may not be the best approach, as either there is a clean separation between \(Y_c\) and \(Y_t\), in which
case this method isn't necessary, or the data is extremely noisy, so that it's unlikely to be able to tease out the subtle effect the method is
looking at.

\(\eta\), our main parameter of interest, is restricted to \((-1,2)\) as described in Section \ref{p1:calculations:problem:reasonable}. We either
iterate over a grid on that range, to examine coverage as \(\eta\) varies, or we draw it randomly from \(U(-1,2)\). We also force \(\eta=0\)
occasionally to remove it from the model entirely.

Finally, \(\sigma^2\), the variance of the error on the relationship between \(X\) and \(Y\). We assumed standardization of \(X\) and \(Y\) such that
\(\sigma^2\) is 1.

With the parameters specified, we can generate the response. Akin to the method, we do this in two stages.

First, we generate \(Y_c\) in the entire population, using
\begin{equation}
  Y_c = X\beta_c.
  \label{p1:datagen-control}
\end{equation}

Recall again that \(\beta_c \in \R_q\), but some subset of those \(\beta_c\) can be zero, so that \(Y_c\) is truly generated by \(p \leq q\) subset of
\(X\).

Next, we generate \(Y_t\),
\begin{equation}
  Y_t = Y_c + \tau Z + \eta ZY_c.
  \label{p1:eq:datagen-treat}
\end{equation}

Finally, we set \(Y=Y_\textrm{obs}\) by
\begin{equation}
  Y_\textrm{obs} = Y_tZ + Y_c(1 - Z) + \epsilon,
  \label{p1:eq:datagen-obs}
\end{equation}
where \(\epsilon \sim N(0,\sigma^2)\). \(Z\) defines treatment status, specifically that \(Z_i = 1\) implies membership in the treatment group and
\(Z_i = 0\) the control group.

If \(\eta = 0\), then (\ref{p1:eq:datagen-treat}) is simplified to
\begin{equation}
  Y_t = Y_c + \tau Z
\end{equation}
so that there is only an additive treatment effect.

If \(\tau\) is also set to \(0\), then \(Y_t = Y_c\) and the treatment is completely ineffective.

We add the error to \(Y_\textrm{obs}\) instead of \(Y_c\) and \(Y_t\) to ensure homogeneity of the error; it should be easy to see that if we added
the error to \(Y_c\) or both \(Y_c\) and \(Y_t\), the error variance could differ drastically in the treatment and control groups.

Of course, \(Y_t\) and \(Y_c\) are discarded, and \(Y_\textrm{obs}\) is treated as the only observable response.

\subsection{General Results}
\label{p1:simulations:results}

\subsubsection{Choice of Covariance Estimate}
\label{p1:simulations:results:covariance}

In Section \ref{p1:calculation:testing}, we choose to use an estimator of \(\eta\)'s standard deviation, (\ref{p1:eq:calc-test-se}), based upon a
hybrid estimator (\ref{p1:eq:est-meth-test-hybrid2}). We justify that choice now. Recall that
\(\tilde{\lambda}_0 = (\hat{\beta}_c, \tilde{\tau}_0, \eta_0)\) is under the constraint \(H_0: \eta = \eta_0\) with \(\tilde{\tau}_0\) being the least
squares estimate of \(\tau\) under that hypothesis, and that \(\hat{\lambda} = (\hat{\beta}_c, \hat{\tau}, \hat{\eta})\) is unconstrained. We have the
choice between using \(\tilde{\lambda}_0\) over \(\hat{\lambda}\) in both bread and meat (\ref{p1:eq:est-meth-test-model}); neither bread nor meat
(\ref{p1:eq:est-meth-test-empirical}); or only meat (\ref{p1:eq:est-meth-test-hybrid1}) or only bread (\ref{p1:eq:est-meth-test-hybrid2}). We utilize
the confidence interval instead of the hypothesis test to make this decision as the confidence interval by inversion contains, the hypothesis test of
\(H_0: \eta = 0\). We perform simulations using each variation of estimate to examine coverage. The results are presented in Figure
\ref{p1:fig:sim:choosevar}.

<<echo=FALSE, results="asis", fig.width=5, fig.height=5, fig.cap="Choosing the version of the covariance estimator. Model-based refers to (\\ref{p1:eq:est-meth-test-model}), empirical to (\\ref{p1:eq:est-meth-test-empirical}) and the two hybrids are (\\ref{p1:eq:est-meth-test-hybrid1}) and (\\ref{p1:eq:est-meth-test-hybrid2}) respectively. Estimator Hybrid 2 obtains 95\\% proper coverage. \\label{p1:fig:sim:choosevar}", fig.scap="Choosing version of covariance estimate, coverage.">>=
source("code/choose_version_of_var.R")

# Generate average wider-ness of confidence interval.
wider <- round((mean(chooseVarSave$median.width[chooseVarSave$form == 1]/chooseVarSave$median.width[chooseVarSave$form == 4]) - 1)*100)

plot(chooseVarSave$covered[chooseVarSave$form == 4] ~ chooseVarSave$truth[chooseVarSave$form == 4], type = 'l', ylim = c(.75, 1),
     xlab = expression(eta), ylab = "Confidence Interval Coverage")
lines(chooseVarSave$covered[chooseVarSave$form == 1] ~ chooseVarSave$truth[chooseVarSave$form == 1], col = 2)
lines(chooseVarSave$covered[chooseVarSave$form == 2] ~ chooseVarSave$truth[chooseVarSave$form == 2], col = 3)
lines(chooseVarSave$covered[chooseVarSave$form == 3] ~ chooseVarSave$truth[chooseVarSave$form == 3], col = 4)
mtext("n = 100", side = 3, at = -.75)
abline(h = .95, lty = 2, col = 'lightgrey')
legend("bottomleft", legend = c("Model-based", "Empirical", "Hybrid 1", "Hybrid 2"), col = c(2:4, 1), lty = 1)
@

Neither the empirical version nor the first hybrid obtain proper coverage, which are the variations using \(\hat{\lambda}\) in the bread. The
model-based version and the second hybrid, which use \(\tilde{\lambda}_0\) in the bread, obtain proper coverage, although the fully model-based
version shows overcoverage. Using \(\tilde{\lambda}_0\) in the bread adds stability to the covariance estimates, which is important because the bread
is inverted.

The second hybrid is the best choice for three reasons. First, it obtains proper coverage without overcoverage. Second, the second hybrid estimator
allows the covariance estimate to be quadratic in \(\eta_0\), ensuring with simplicity that (\ref{p1:eq:inv-thetest}) defines a confidence
interval. Finally, in lieu of power analysis we compare the two versions in terms of the width of the generated confidence interval. Figure
\ref{p1:fig:sim:choosevar2} shows that the confidence intervals generated by the model-based version are \Sexpr{wider}\% wider on average.

<<echo=FALSE, results="asis", fig.width=5, fig.height=5, fig.cap=paste0("Median width of confidence intervals generated using model-based, (\\ref{p1:eq:est-meth-test-model}), and the second hybrid, (\\ref{p1:eq:est-meth-test-hybrid2}), which both obtained proper coverage. The model-based version averages ", wider,"\\% wider confidence intervals. \\label{p1:fig:sim:choosevar2}"), fig.scap="Choosing version of covariance estimate, width.">>=
source("code/choose_version_of_var.R")

whichwidth <- chooseVarSave$median.width
plot(whichwidth[chooseVarSave$form == 4] ~ chooseVarSave$truth[chooseVarSave$form == 4], type = 'l',
       xlab = expression(eta), ylab = "Confidence Interval Median Width", ylim = c(min(whichwidth), max(whichwidth)))
lines(whichwidth[chooseVarSave$form == 1] ~ chooseVarSave$truth[chooseVarSave$form == 1], col = 2)
legend("bottomright", legend = c("Model-based", "Hybrid 2"), col = 2:1, lty = 1)
mtext("n = 100", side = 3, at = -.75)
@

Therefore we have justified our choice in Section \ref{p1:calculation:testing} of using
\begin{equation}
  \sigma^2_{\tilde{\lambda}_0}(\hat{\eta}) = B(\tilde{\lambda}_0)^{-1}M_n(\hat{\lambda})B(\tilde{\lambda}_0)^{-T}.
\end{equation}

\subsubsection{Finite Sample Rule of Thumb for \(q\)}
\label{p1:simulations:results:chooseq}

We describe this as a rule of thumb to indicate that there is a certain amount of judgment behind the choice of the threshold; a more rigorous review
of the topic may reveal a tighter rule. For our simulation results, this casual result is sufficient.

The asymptotic rule in \citet{he2000parameters} is that \(q^2 \log(q) = o(n)\). We consider a slower growth rate, \(\frac{q^2 \log(q)^2}{n} < C\) and
choose some \(C\) such that the rule holds. To determine the choice of \(C\), we iterate over choices of \(n\) and \(C\), and perform equivalence
testing. Equivalence testing is used in clinical trials to test whether a new drug is not appreciably worse than an existing drug, as opposed to
traditional superior hypothesis testing which considers whether the new drug is better than the existing.\citep{walker2011understanding} Similar to
that design, we wish to choose the largest \(C\) for which coverage is not significantly lower than \(1 - \alpha\).

For our simulation, we run 10,000 repetitions of each \((n, C)\) pair. We set a threshold of .1\% as an equivalence region. This yields a rejection
value of 94.7\%.

The results of this simulation are displayed in Figure \ref{p1:fig:sim:chooseq}. The green squares have coverage above \(.947\), and red
below. Therefore, we choose \(C\) = 2.5, resulting in the aforementioned \(q\) = 7 and \(q\) = 17 for \(n\) = 100 and \(n\) = 1,000 respectively. It
is likely there is another rule which is less strict as \(n\) increases, shown by the lack of failures for \(n\) = 500 or 1,000.

<<echo=FALSE, results="asis", fig.width=4.5, fig.height=3, fig.cap="Simulation testing choices of $C$ to establish the largest values of $q$. Each box represents 10,000 repetitions using the $n$ and $C$ combination. Red boxes reject the non-inferiority null that coverage with the $(n, C)$ pair is no worse than 94.7\\%. \\label{p1:fig:sim:chooseq}", fig.scap="Finite sample size rule of thumb">>=
source("code/nvsp.R")

bigsave <- bigsave[, 1:11]

image(t(apply(bigsave, 2, rev)) > .947, col = c("red", "green"), axes = FALSE, ylab = "Choices of n")
axis(3, at = seq(0, 1, length = ncol(bigsave)), labels = rep("", ncol(bigsave)))
axis(3, at = seq(0, 1, length = ncol(bigsave)/2 + .5), labels = 1:(ncol(bigsave)/2 + .5), tck = -.07)
axis(2, at = seq(0, 1, length = 5), labels = rev(rownames(bigsave)), las = 2)
mtext("Choices of C", side = 3, padj = -3.5)
grid(ncol(bigsave), nrow(bigsave), col = 'black')
@

\subsubsection{Hypothesis Test}
\label{p1:simulations:results:test}

We perform a level \(\alpha = .95\) hypothesis test against \(H_0: \eta = 0\) by first generating data as described above, forcing \(\eta = 0\), to
ensure proper Type 1 error. The results for 1,000 runs at \(n\) = 100 and \(n\) = 1,000 are in Table \ref{p1:tab:sim:ht}. The corrected PBPH method
obtains proper coverage levels.

<<echo=FALSE, results="asis">>=
source("code/htestsim.R")

s <- paste0(sprintf("%.1f",100*c(mean(htestsim.save$pval100 < .05), mean(htestsim.save$pval1000 < .05))), "%")
s <- data.frame(s[1], s[2])
rownames(s) <- "Percentage Rejection"
colnames(s) <- c("n = 100", "n = 1,000")
print(xtable(s,
             caption = c("Percentage of tests rejecting over 1,000 iterations.", "PBPH overall performance"),
             label = "p1:tab:sim:ht",
             align = c("l","c","c")),
       comment = FALSE)
@

\subsubsection{Coverage Results}
\label{p1:simulations:results:coverage}

We've shown that the corrected PBPH method provides proper coverage on a level \(\alpha\) test of the null that \(\eta = 0\). The next step is to
examine confidence interval coverage when \(\eta\) is not zero.

Table \ref{p1:tab:sim:overallcoverage} shows coverage percentages across different values of true \(\eta\). We obtain proper approximate \(95\%\)
coverage across all reasonable values of \(\eta\) (again, see Section \ref{p1:calculations:problem:reasonable} for the rationale for the reasonable
values) for both sample sizes. The negative bias discussed in Section \ref{p1:motivation:modelfit} appears, though muted in the larger sample size.

Note that the overall coverage is for both desirable (continuous) and undesirable (disjointly infinite) shapes of confidence regions combined. Table
\ref{p1:tab:sim:sepcoverage} shows coverage in each shape of the region. Amongst continuous confidence intervals, proper coverage is
maintained. Amongst disjointly infinite confidence regions, we see over-coverage. Overall, in only a handful of the 7,000 total runs did we observe
the situation where a disjointly infinite confidence region failed to cover the true value of \(\eta\).

Table \ref{p1:tab:sim:sepcoverage} does not include an entry corresponding to \(n\) = 1,000 because in the larger sample size setting, we did not
encounter a single disjointly infinite confidence region in our simulations.

<<echo=FALSE, results="asis">>=
source("code/coverage_by_type100.R")
source("code/coverage_by_type1000.R")

s <- cbind(cov.by.type100.save[,c(1,2,5)], cov.by.type1000.save[,c(2,5)])

# Double \\ is escape
s[,3] <- paste0(sprintf("%.1f", round(100*s[,3], 3)), "\\%")
s[,5] <- paste0(sprintf("%.1f", round(100*s[,5], 3)), "\\%")

colnames(s) <- c("$\\eta$", rep(c("$\\hat{\\eta}$", "Coverage"), 2))

#digits and align is 6 because of row.names, they'll be dropped on print.
xt <- xtable(s,
             caption = c("Coverage of $\\eta$, combined all shapes of confidence regions. For data generation, when $n$ = 100 there are $q$ = 7 parameters and $p$ = 3, and for $n$ = 1,000, $q$ = 17 and $p$ = 6.", "PBPH performance by $\\eta$"),
             label = "p1:tab:sim:overallcoverage",
             digits <- c(1,1,2,1,2,1),
             align="cc|cc|cc")

# Sanitize allows math in column names
print(xt,
      include.rownames = FALSE,
      include.colnames = FALSE,
      sanitize.text.function=function(x){x},
      comment = FALSE,
      add.to.row = list(pos = list(-1),
                        command = paste( "& \\multicolumn{2}{c}{$n$ = 100} & \\multicolumn{2}{c}{$n$ = 1,000} \\\\\n",
                        " $\\eta$ & $\\hat\\eta$ & Coverage & $\\hat\\eta$ & Coverage  \\\\\n") ) )
@

<<echo=FALSE, results="asis">>=
source("code/coverage_by_type100.R")

s <- cov.by.type100.save[,-c(2:5)]
s$cont <- s$cont_un + s$cont_cov
s$disjoint <- s$disjoint_un + s$disjoint_cov
s <- s[,c("truth", "cont", "cont_per", "disjoint", "disjoint_per")]

s$cont_per <- paste0(round(s$cont_per*100), "\\%")
s$disjoint_per <- paste0(round(s$disjoint_per*100), "\\%")

colnames(s) <- c("$\\eta$", rep(c("Count", "Coverage"), 2))

xt <- xtable(s,
             caption = c("Coverage of $\\eta$ over several values, separated by shape of confidence regions. $n=100$ with $q=7$ parameters and $p=3$ used in data generation.", "PBPH performance by $\\eta$ and type"),
             label = "p1:tab:sim:sepcoverage",
             digits <- c(1,1,0,0,0,0),
             align="cc|cc|cc")

# Sanitize allows math in column names
print(xt,
      include.rownames=FALSE,
      include.colnames=FALSE, # manually define below
      sanitize.text.function=function(x){x},
      comment = FALSE,
      add.to.row = list(pos = list(-1),
                        command = paste( "& \\multicolumn{2}{c}{Continuous} & \\multicolumn{2}{c}{Disjointly Infinite} \\\\\n",
                                          " $\\eta$ & Count & Coverage & Count & Coverage  \\\\\n") ) )
@

\subsection{Infinite Confidence Regions}
\label{p1:simulations:infci}

While in the previous section we considered both finite and infinite confidence intervals as similar, we separate them here to examine the
relationship between first stage model fit and the shape of the confidence region. We restrict ourselves to smaller sample sizes as we never observed
the disjointly infinite confidence regions in the \(n\) = 1,000 setting. We simulate data as described above with \(n\) = 100, but allow \(\eta\) to
be drawn randomly from \(U(-1,2)\).

<<echo=FALSE, fig.cap="Comparison of $F$-statistic (on the log scale) across the different shapes of the confidence region.\\label{p1:fig:sim:modfit}", fig.scap="Fit vs type">>=
source("code/firststagefit.R")
@

To measure model fit, we will consider the ANOVA associated with the first stage regression, and specifically its \(F\)-statistic. We base our choice
on model fit measure due to guidance from the instrumental variables literature, particularly the determination of a weak instrument in the two-stage
least squares instrumental variables procedure. For example, \citet{stock2005testing} derives a critical value to test directly against the first-stage
\(F\)-statistic.

To ease interpretation, we'll look at the \(F\)-statistic on the log-scale. The results are presented in Figure \ref{p1:fig:sim:modfit}. We can see
that significance in the \(F\)-test is strongly associated with a finite confidence interval. Infinite confidence regions are almost entirely
associated with a failure to reject the \(F\)-test. This corresponds to intuition, as if the first stage model fit is weak, we have little confidence
in any second stage results.

Unfortunately, disjoint confidence regions are often associated with significant p-values, although not as significant as the finite confidence
intervals. It would be convenient if the disjoint confidence regions were associated with poor first stage model fit, but since we are able to reject
some values in the second stage, we must have some power at that stage. It follows that for the second stage to have power, the first stage must have
had some power as well.

We looked at the additional measure of fit, \(R^2\), and found an extremely similar pattern.

Finally, and perhaps not surprisingly given the results thus far, if the first-stage model does not identify any non-zero predictors, obtaining a
finite confidence interval is extremely unlikely.

\subsubsection{An Attempt at Narrowing Infinite Confidence Intervals}

As defined in Section \ref{p1:calculation:testing}, for each \(H_0: \eta = \eta_0\) which is tested, the parameters under the null hypothesis are
\(\tilde{\lambda}_0 = (\hat{\beta}_c, \tilde{\tau}_0, \eta_0)\), where \(\tilde{\tau}_0\) is the least squares estimate of \(\tau\) under
\(H_0\). Consider the model for obtaining \(\tilde{\tau}_0\), fit only on the treated group,
\begin{equation}
  \begin{split}
    Y - X\beta_c &= \tau_0 + \eta_0 X\beta_c + \epsilon.\\
    Y - (1 + \eta_0)X\beta_c &= \tau_0 + \epsilon.
  \end{split}
\end{equation}

In other words, \(\tilde{\tau}_0\) is the expected value of \(Y - (1 + \eta_0)X\beta_c\) among the treated. In the limits, we have that
\(\lim_{\eta_0 \to \infty} \tilde{\tau}_0 = -\infty\) and \(\lim_{\eta_0 \to -\infty} \tilde{\tau}_0 = \infty\). If we consider \(\tau\) as a nuisance
parameter when testing hypotheses about \(\eta\), then an approach introduced in \citet{berger1994p} suggests bounding \(\tilde{\tau}_0\) by a wide
confidence interval and appropriately adjusting the subsequent p-values. Assume that this approach would work, that is, that bounding
\(\tilde{\tau}_0\) by a wide (finite) confidence interval results in confidence intervals being finite. Because we are in theory (though not in
practice) generating hypothesis tests over all values of \(\eta_0\), and the asymptotic relationship between \(\eta_0\) and \(\tilde{\tau}_0\) is as
described, bounding \(\tilde{\tau}_0\) at any finite limits will equally result in finite confidence intervals for \(\hat{\eta}\), and we need not
restrict the bounds to the confidence interval of \(\tilde{\tau}_0\).

Assume we bound \(\tilde{\tau}_0 \in [u, l]\), that \(\eta_l\) and \(\eta_u\) solve \(\tilde{\tau}_0 = l\) and \(\tilde{\tau}_0 = u\) respectively,
and that \(\eta_u \leq \eta_l\) without loss of generality. With this modification, \(w_\alpha(\eta_0)\) is no longer necessarily continuous, as it
may be different in the three ranges \((-\infty, \eta_u)\), \((\eta_u, \eta_l)\) and \((\eta_l, \infty)\). However, the shape of \(w_\alpha(\eta_0)\)
is still quadratic in \(\eta_0\) in each range.

The conjecture is that bounding \(\tilde{\tau}_0\) would reduce the incidence of infinite confidence regions. If we rewrite \(w_\alpha(\eta_0)\) as
\(a\eta_0^2 + b\eta_0 + c\) where \(a\), \(b\) and \(c\) are some function of the data, the critical value of the \(\chi^2\) distribution,
\(\hat{\beta}_c\), \(\tilde{\tau}_0\) and \(\hat{\eta}\), then if \(a > 0\), we must have a finite confidence region (because empty confidence regions
are not possible, see the argument in the Section \ref{p1:calculation:testinversion}). However, we can easily find counter-examples. Table
\ref{p1:fig:sim:counter} shows a few. In each case, \(n\) = 100 and the data is generated as described in Section \ref{p1:simulations:datagen}. Each
simulation run resulted in an infinite confidence region and should be eligible to be adjusted by this approach. In each counterexample case, we see
the coefficient on \(\eta_0^2\) remain negative even as \(\tilde{\tau}_0\) is bound.

<<echo=FALSE, results='asis'>>=
source("code/fixtausim.R")

getsign <- function(x) ifelse(sign(x) > 0, "+", "-")

ns <- t(apply(fixtausim.save, 1, function(xs) {
  c(paste0(xs[3], "\\(\\eta_0^2\\) ", getsign(xs[2]), " ", abs(xs[2]),
           "\\(\\eta_0\\) ", getsign(xs[1]), " ", abs(xs[1])),
    paste0(xs[6], "\\(\\eta_0^2\\) ", getsign(xs[5]), " ", abs(xs[5]),
           "\\(\\eta_0\\) ", getsign(xs[4]), " ", abs(xs[4])),
    paste0(xs[9], "\\(\\eta_0^2\\) ", getsign(xs[8]), " ", abs(xs[8]),
           "\\(\\eta_0\\) ", getsign(xs[7]), " ", abs(xs[7])))
}))

colnames(ns) <- c("Left-asymptotic region", "Center region; unbound \\(\\tilde{\\tau}\\)", "Right-asymptotic region")

print(xtable(ns,
             caption = c("Showing counterexamples to bounding \\(\\tilde{\\tau}_0\\). Each equation is \\(w_\\alpha(\\eta_0)\\), and the middle column
                          represents both the unbounded version and the center region while bounding. The left- and the right-asymptotics are with the
                          bounding. Bounding does not induce the coefficient on the squared term to be positive, and thus does not stop infinite
                          confidence regions.", "Counterexamples to bounding \\(\\tilde{\\tau}\\)"),
             label = "p1:fig:sim:counter",
             align = "cccc"),
      include.row.names = FALSE,
      sanitize.text.function=function(x){x})

@

\subsection{Underfitting and Overfitting in the First Stage}
\label{p1:simulations:underover}

We've shown that fitting the first stage model well is important to being able to identify the additional treatment effect of the predicted response
to control, if any. We now examine the effects of over-fitting on the first stage.

To start, we fix \(n=100\) and \(q=7\). However, when generating \(Y\), we allow only the first four coefficients to be non-zero, hence \(p=4\). This
allows us to compare three difference models,
\begin{align}
  Y &= \beta_1X_1, \label{p1:eq:sim-underover-under}\\
  Y &= \sum_{j=1}^{4} \beta_jX_j, \label{p1:eq:sim-underover-oracle}\\
  Y &= \sum_{j=1}^{7} \beta_jX_j. \label{p1:eq:sim-underover-over}
\end{align}

Clearly, the first model is underfit and the last model is overfit, while the middle model is an oracle model. We draw \(\tau\) and \(\eta\) randomly,
to disallow coverage being affected by the specific choices of \(\tau\) and \(\eta\). We repeated the above 1,000 times. In Table
\ref{p1:fig:sim:overunder1} we show overall coverage in each model and Table \ref{p1:fig:sim:overunder2} shows the distribution of the shapes of the
confidence region by model.

<<echo=FALSE, results='asis'>>=
source("code/overunder.R")
reps <- 1000
outtable <- round(rbind(100*c(mean(overunder.save[["saveunder"]]$covered),
                              mean(overunder.save[["saveoracle"]]$covered),
                              mean(overunder.save[["saveover"]]$covered)),
                        cbind(table(overunder.save[["saveunder"]]$type),
                              table(overunder.save[["saveoracle"]]$type),
                              table(overunder.save[["saveover"]]$type))/reps*100))

outtable <- data.frame(matrix(paste0(outtable, "\\%"), ncol=3))
rownames(outtable) <- c("Coverage", "Continuous", "Disjoint")
colnames(outtable) <- c("Underfit", "Oracle", "Overfit")

xt1 <- xtable(outtable[1,,drop=FALSE],
             caption = c("Coverage of confidence regions based upon (\\ref{p1:eq:sim-underover-under}), (\\ref{p1:eq:sim-underover-oracle}) and (\\ref{p1:eq:sim-underover-over}) respectively, regardless of the shape of the confidence region.", "Under- vs Over-fitting performance, overall"),
             label = "p1:fig:sim:overunder1",
             align="cccc")

xt2 <- xtable(outtable[-1,,drop=FALSE],
             caption = c("Percentage of each type of confidence region found in simulation with each version of the model.", "Under- vs Over-fitting performance, by type"),
             label = "p1:fig:sim:overunder2",
             align="cccc")

print(xt1,
      sanitize.text.function=function(x){x},
      comment = FALSE)
print(xt2,
      sanitize.text.function=function(x){x},
      comment = FALSE)

@
There are two notable conclusions to draw from this simulation.

First, underfit models in the first stage yield a slightly conservative coverage compared to oracle and overfit models. This is likely related to the
extremely large percentage of infinite confidence intervals, which have no chance of rejecting the null hypothesis.

Secondly, within the restrictions on the dimension of \(q\) in Section \ref{p1:simulations:results:chooseq}, there is no penalty for overfitting, with
equivalent coverage and very slightly less common infinite confidence regions. When overfitting beyond the limits of those restrictions, we did see a
lack of proper coverage on other simulations.

\subsection{Returning to \citet{gine2012credit}}
\label{p1:simulations:gine}

We return now to our motivating example, the microlending paper \citet{gine2012credit}. Recall that in this paper, the authors used an uncorrected PBPH
method to estimate the effect of fingerprinting after stratifying the subjects into quintiles based upon their predicted loan repayment.

<<echo=FALSE>>=
# generates gine_naive_coverage
source("code/gine_naive_coverage.R")
gine_naive_coverage <- mean(gine_sim_res[,1] < .05)*100
gine_corr_coverage  <- mean(gine_sim_res[,2] < .05)*100
@

First, we repeat the simulation described in Section \ref{p1:motivation:computational}, where we split the control group into faux treatment and
control groups. Then, knowing that all treatments effects are zero on average, we repeated the uncorrected PBPH method and rejected the null in
\Sexpr{gine_naive_coverage}\% of the runs. In those same runs, utilizing the corrected PBPH method, we rejected the null in only
\Sexpr{gine_corr_coverage}\% of all runs

Moving away from simulations, we compare the published results with the results from a corrected PBPH method.

As Table \ref{p1:tab:mot:ex:fingerprint} cited, the reported coefficient for an interaction term between fingerprinting and a continuous predicted
repayment was -0.807 with a standard error of 0.120. This standard error was the result of a bootstrap estimation procedure, and involved a model
which included club effects (a club being a group of individual participants who assume joint risk for the loans in exchange for improved rates). Our
results, contained in Table \ref{p1:tab:sim:gine:bounds}, directly estimate the standard error, and discards the club level effect for simplicity, so
our results are slightly different than those published.


<<echo=FALSE, results="asis">>=
source("code/gine_corrected.R")

xt <- xtable(ginecorr,
             caption = c("Comparison of uncorrected and corrected confidence intervals. Results differ from published results in \\citet{gine2012credit} in Table \\ref{p1:tab:mot:ex:fingerprint} due to simplifying model.", "PBPH vs results from \\citet{gine2012credit}"),
             label = "p1:tab:sim:gine:bounds",
             align="cccc",
             digits=3)

print(xt,
      sanitize.text.function=function(x){x},
      comment = FALSE)
@

This does not change the results of the paper; with the corrected PBPH we still reject the null hypothesis \(H_0: \eta = 0\).

Notice that the standard error from the corrected method is larger and the confidence interval wider, and the corrected confidence interval is not
centered around the point estimate. Theses are all behaviors predictable by our method; the first two due to considering the measurement error on the
predicted response to control, and the last due to the creation of a non-Wald-style confidence interval.
